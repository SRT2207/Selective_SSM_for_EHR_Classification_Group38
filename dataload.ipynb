{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SRT2207/Structured_SSM_for_EHR_Classification_Group38/blob/main/dataload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f-aUrA4N_PZ3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IleomlXD_Qt-"
   },
   "source": [
    "## Load data and package requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "534gszGSA-kq",
    "outputId": "0deeb5fc-bbff-4439-a44c-d70ebb69af51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118, https://data.pyg.org/whl/torch-2.2.0+cu118.html\n",
      "Collecting torch==2.2.0 (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 2))\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp310-cp310-linux_x86_64.whl (811.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.7/811.7 MB\u001b[0m \u001b[31m812.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch_scatter (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 4))\n",
      "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting numpy==1.25.1 (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 5))\n",
      "  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting tqdm==4.66.2 (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 6))\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.1.4 (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 7))\n",
      "  Downloading pandas-1.1.4.tar.gz (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: h5py==3.12.1 in /usr/local/lib/python3.10/dist-packages (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 8)) (3.12.1)\n",
      "Collecting matplotlib==3.8.2 (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 9))\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 10)) (0.1.7)\n",
      "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 11)) (8.1.7)\n",
      "Collecting scikit-learn==0.24.2 (from -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt (line 12))\n",
      "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install -r Structured_SSM_for_EHR_Classification_Group38/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ropSVNL6BJHS",
    "outputId": "bae4136c-4e2a-4564-f168-ec8d68a7a26d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames created: ['normalization_physionet2012_5', 'normalization_physionet2012_3', 'normalization_physionet2012_2', 'normalization_physionet2012_1', 'normalization_physionet2012_4']\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the P12data directory\n",
    "data_path = 'Structured_SSM_for_EHR_Classification_Group38/P12data/'\n",
    "\n",
    "# List all .json files in the directory\n",
    "json_files = [f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "\n",
    "# Load each JSON file into a DataFrame with the same name as the file\n",
    "json_df = {}\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        # Normalize JSON data into a DataFrame\n",
    "        df = pd.json_normalize(data)\n",
    "        # Store in a dictionary with the key as the filename\n",
    "        json_df[file.replace('.json', '')] = df\n",
    "\n",
    "# Display the keys of the dictionary\n",
    "print(\"DataFrames created:\", list(json_df.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ds9zmBuxDpwe",
    "outputId": "870a32a1-e12e-4fea-de3c-45e87f356e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames created: ['test_physionet2012_1', 'train_physionet2012_1', 'validation_physionet2012_1', 'train_physionet2012_3', 'validation_physionet2012_3', 'test_physionet2012_3', 'validation_physionet2012_5', 'test_physionet2012_5', 'train_physionet2012_5', 'train_physionet2012_2', 'validation_physionet2012_2', 'test_physionet2012_2', 'validation_physionet2012_4', 'test_physionet2012_4', 'train_physionet2012_4']\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store DataFrames with the name of the .npy file as the key\n",
    "dataframes = {}\n",
    "\n",
    "# List all .tar.gz files in the directory\n",
    "tar_files = [f for f in os.listdir(data_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# Loop through each .tar.gz file, extract, and load .npy data into DataFrames\n",
    "for tar_file in tar_files:\n",
    "    tar_path = os.path.join(data_path, tar_file)\n",
    "    extract_folder = os.path.join(data_path, tar_file.replace('.tar.gz', ''))\n",
    "\n",
    "    # Create directory for extracted files\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "    # Extract the tar.gz file\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_folder)\n",
    "\n",
    "    # Load each .npy file in the extracted folder\n",
    "    for root, _, files in os.walk(extract_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Check if the file is a .npy file\n",
    "            if file.endswith('.npy'):\n",
    "                # Load the .npy file with allow_pickle=True\n",
    "                array_data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "                # Convert to DataFrame\n",
    "                df = pd.DataFrame(array_data)\n",
    "\n",
    "                # Store the DataFrame in the dictionary using the .npy file's name (without extension) as the key\n",
    "                file_name_without_extension = file.replace('.npy', '')\n",
    "                dataframes[file_name_without_extension] = df\n",
    "\n",
    "# Display the keys of the dictionary to verify the DataFrames are named correctly\n",
    "print(\"DataFrames created:\", list(dataframes.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JelurDXkEYp-"
   },
   "outputs": [],
   "source": [
    "# for name, df in dataframes.items():\n",
    "#     print(f\"Contents of DataFrame '{name}':\")\n",
    "#     display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1y5Vh_HNyf6"
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yBG2CP7hEbHX"
   },
   "outputs": [],
   "source": [
    "# Define the path to the models directory\n",
    "models_path = 'Structured_SSM_for_EHR_Classification_Group38/models'\n",
    "\n",
    "# Add the models directory to the system path\n",
    "sys.path.append(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwZKA0kTNvAf",
    "outputId": "94fc6a80-d576-4608-e071-0e4286c2b6b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'GRUD', 'GRUDCell', 'GRUDModel', 'List', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'exp_relu', 'generate_masks', 'get_activation', 'jit', 'masked_max_pooling', 'masked_mean_pooling', 'nn', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import grud  # Import the module\n",
    "\n",
    "# List all functions and classes in grud model\n",
    "print(dir(grud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZhIHUcGOIL_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUNWkwPZfFbQUpZMDDV2Fi",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
