{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File: AttentionVisualization.ipynb\n",
    "---------------------------------\n",
    "Visualize the attention layers of transformer models for interpretability.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24876a0c6020df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from bertviz import head_view, model_view\n",
    "from bertviz.neuron_view import show\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import utils\n",
    "\n",
    "\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "\n",
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from odyssey.data.dataset import FinetuneDataset\n",
    "from odyssey.data.tokenizer import ConceptTokenizer\n",
    "from odyssey.evals.prediction import load_finetuned_model, predict_patient_outcomes\n",
    "from odyssey.models.model_utils import (\n",
    "    load_finetune_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac09785d1fdc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    \"\"\"Save the configuration arguments.\"\"\"\n",
    "\n",
    "    model_path = \"checkpoints/best.ckpt\"\n",
    "    vocab_dir = \"odyssey/data/vocab\"\n",
    "    data_dir = \"odyssey/data/bigbird_data\"\n",
    "    sequence_file = \"patient_sequences_2048_mortality.parquet\"\n",
    "    id_file = \"dataset_2048_mortality.pkl\"\n",
    "    valid_scheme = \"few_shot\"\n",
    "    num_finetune_patients = \"20000\"\n",
    "    label_name = \"label_mortality_1month\"\n",
    "\n",
    "    max_len = 2048\n",
    "    batch_size = 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e21258ced06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ConceptTokenizer(data_dir=args.vocab_dir)\n",
    "tokenizer.fit_on_vocab(with_tasks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf63dc4242a163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_finetuned_model(args.model_path, tokenizer)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb295710f64bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune, fine_test = load_finetune_data(\n",
    "    args.data_dir,\n",
    "    args.sequence_file,\n",
    "    args.id_file,\n",
    "    args.valid_scheme,\n",
    "    args.num_finetune_patients,\n",
    ")\n",
    "\n",
    "fine_tune.rename(columns={args.label_name: \"label\"}, inplace=True)\n",
    "fine_test.rename(columns={args.label_name: \"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FinetuneDataset(\n",
    "    data=fine_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(test_dataset, [85, 89]),  # 85 and 88 are small\n",
    "    batch_size=args.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6f3658dda5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = next(iter(test_loader))\n",
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025c033efc9baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict_patient_outcomes(patient, model)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa8309b2650820",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.decode(patient[\"concept_ids\"].squeeze(0).cpu().numpy()).split(\" \")\n",
    "truncate_at = patient[\"attention_mask\"].sum().numpy()\n",
    "attention_matrix = output[\"attentions\"]\n",
    "last_attention_matrix = attention_matrix[-1].detach()\n",
    "# batch_size x num_heads x max_len x max_len    x num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9203f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_attention_matrix = []\n",
    "\n",
    "for i in range(len(attention_matrix)):\n",
    "    truncated_attention_matrix.append(\n",
    "        attention_matrix[i][:, :, :truncate_at, :truncate_at],\n",
    "    )\n",
    "\n",
    "truncated_attention_matrix = tuple(truncated_attention_matrix)\n",
    "truncated_tokens = tokens[:truncate_at]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(\n",
    "    attention_weights,\n",
    "    patient,\n",
    "    special_tokens,\n",
    "    tokenizer,\n",
    "    truncate=False,\n",
    "    only_cls=False,\n",
    "    top_k=10,\n",
    "):\n",
    "    # Convert attention tensor to numpy array and squeeze the batch dimension\n",
    "    concept_ids = patient[\"concept_ids\"].squeeze(0).cpu().numpy()\n",
    "    attention_weights = attention_weights.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Truncate attention weights if specified\n",
    "    if truncate:\n",
    "        truncate_at = patient[\"attention_mask\"].sum().numpy()\n",
    "        attention_weights = attention_weights[:, :truncate_at, :truncate_at]\n",
    "        concept_ids = concept_ids[:truncate_at]\n",
    "\n",
    "    if only_cls:\n",
    "        attention_weights = attention_weights[:, :1, :]\n",
    "\n",
    "    # Average attention weights across heads\n",
    "    attention_weights = attention_weights.mean(axis=0)\n",
    "\n",
    "    # Generate token labels, marking special tokens with a special symbol\n",
    "    x_token_labels = [\n",
    "        f\"{tokenizer.id_to_token(token)}\"\n",
    "        if tokenizer.id_to_token(token) in special_tokens\n",
    "        else str(i)\n",
    "        for i, token in enumerate(concept_ids)\n",
    "    ]\n",
    "    y_token_labels = [\"[CLS]\"]\n",
    "\n",
    "    # Generate hover text\n",
    "    hover_text = [\n",
    "        [\n",
    "            f\"Token {tokenizer.id_to_token(concept_ids[row])} with Token {tokenizer.id_to_token(concept_ids[col])}:\"\n",
    "            f\"Attention Value {attention_weights[row, col]:.3f}\"\n",
    "            for col in range(attention_weights.shape[1])\n",
    "        ]\n",
    "        for row in range(attention_weights.shape[0])\n",
    "    ]\n",
    "\n",
    "    # Generate annotations for special tokens\n",
    "    annotations = []\n",
    "    for i, token in enumerate(concept_ids):\n",
    "        if tokenizer.id_to_token(token) in special_tokens:\n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    x=i,\n",
    "                    y=0.5,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",  # Use 'paper' coordinates for y\n",
    "                    text=tokenizer.id_to_token(token),\n",
    "                    showarrow=False,\n",
    "                    font=dict(color=\"black\", size=10),\n",
    "                    textangle=-90,\n",
    "                    bgcolor=\"red\",\n",
    "                    opacity=0.8,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Plot the attention matrix as a heatmap\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=attention_weights,\n",
    "            x=x_token_labels,\n",
    "            y=y_token_labels,\n",
    "            hoverongaps=False,\n",
    "            hoverinfo=\"text\",\n",
    "            text=hover_text,\n",
    "            colorscale=\"YlGnBu\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Attention Visualization\",\n",
    "        xaxis_nticks=len(concept_ids),\n",
    "        yaxis_nticks=len(y_token_labels),\n",
    "        xaxis_title=\"Token in Input Sequence\",\n",
    "        yaxis_title=\"Token in Input Sequence\",\n",
    "        annotations=annotations,\n",
    "        xaxis_tickangle=-90,\n",
    "    )\n",
    "\n",
    "    # Print top k tokens with their attention values\n",
    "    top_k_indices = np.argsort(-attention_weights, axis=None)[:top_k]\n",
    "    top_k_values = attention_weights.flatten()[top_k_indices]\n",
    "    top_k_indices = np.unravel_index(top_k_indices, attention_weights.shape)\n",
    "\n",
    "    for idx in range(len(top_k_indices[0])):\n",
    "        token1 = top_k_indices[0][idx]\n",
    "        token2 = top_k_indices[1][idx]\n",
    "        attention_value = top_k_values[idx]\n",
    "        print(\n",
    "            f\"Token {tokenizer.id_to_token(concept_ids[token1])} \"\n",
    "            f\"with Token {tokenizer.id_to_token(concept_ids[token2])}: \"\n",
    "            f\"Attention Value {attention_value:.3f}\",\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Visualize the attention matrix with special tokens\n",
    "special_tokens = [\"[CLS]\", \"[VS]\", \"[VE]\", \"[REG]\"]\n",
    "visualize_attention(\n",
    "    last_attention_matrix,\n",
    "    patient=patient,\n",
    "    special_tokens=special_tokens,\n",
    "    tokenizer=tokenizer,\n",
    "    truncate=True,\n",
    "    only_cls=True,\n",
    "    top_k=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_attention(attention_weights, patient, special_tokens, tokenizer, truncate=False, only_cls=False, top_k=10):\n",
    "#     # Convert attention tensor to numpy array and squeeze the batch dimension\n",
    "#     concept_ids = patient['concept_ids'].squeeze(0).cpu().numpy()\n",
    "#     attention_weights = attention_weights.squeeze(0).cpu().numpy()\n",
    "\n",
    "#     # Truncate attention weights if specified\n",
    "#     if truncate:\n",
    "#         truncate_at = patient['attention_mask'].sum().numpy()\n",
    "#         attention_weights = attention_weights[:, :truncate_at, :truncate_at]\n",
    "#         concept_ids = concept_ids[:truncate_at]\n",
    "\n",
    "#     if only_cls:\n",
    "#         attention_weights = attention_weights[:, :1, :]\n",
    "\n",
    "#     # Average attention weights across heads\n",
    "#     attention_weights = attention_weights.mean(axis=0)\n",
    "\n",
    "#     # Generate token labels, replacing special tokens with their names\n",
    "#     token_labels = [tokenizer.id_to_token(token) if tokenizer.id_to_token(token) in special_tokens else '' for token in concept_ids]\n",
    "\n",
    "#     # Plot the attention matrix as a heatmap\n",
    "#     sns.set_theme(font_scale=1.2)\n",
    "#     plt.figure(figsize=(15, 12))\n",
    "#     ax = sns.heatmap(attention_weights, cmap=\"YlGnBu\", linewidths=.5, annot=False, cbar=True)\n",
    "#     ax.set_title('Attention Visualization')\n",
    "\n",
    "#     # Set custom tick labels\n",
    "#     # ax.set_xticks(np.arange(len(token_labels)) + 0.5)\n",
    "#     # ax.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=10)\n",
    "#     # ax.set_yticks(np.arange(len(token_labels)) + 0.5)\n",
    "#     # ax.set_yticklabels(token_labels, rotation=0, ha='right', fontsize=10)\n",
    "\n",
    "#     ax.set_xlabel('Token in Input Sequence')\n",
    "#     ax.set_ylabel('Token in Input Sequence')\n",
    "\n",
    "#     # Print top k tokens with their attention values\n",
    "#     top_k_indices = np.argsort(-attention_weights, axis=None)[:top_k]\n",
    "#     top_k_values = attention_weights.flatten()[top_k_indices]\n",
    "#     top_k_indices = np.unravel_index(top_k_indices, attention_weights.shape)\n",
    "\n",
    "#     for idx in range(len(top_k_indices[0])):\n",
    "#         token1 = top_k_indices[0][idx]\n",
    "#         token2 = top_k_indices[1][idx]\n",
    "#         attention_value = top_k_values[idx]\n",
    "#         print(f\"Token {tokenizer.id_to_token(concept_ids[token1])} \"\n",
    "#               f\"with Token {tokenizer.id_to_token(concept_ids[token2])}: \"\n",
    "#               f\"Attention Value {attention_value}\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Visualize the attention matrix with special tokens\n",
    "# special_tokens = ['[CLS]', '[VS]', '[VE]', '[REG]']  # Update this list with your actual special tokens\n",
    "# visualize_attention(last_attention_matrix, patient=patient, special_tokens=special_tokens, tokenizer=tokenizer, truncate=True, only_cls=True, top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf327908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model view\n",
    "html_model_view = model_view(\n",
    "    truncated_attention_matrix,\n",
    "    truncated_tokens,\n",
    "    include_layers=[5],\n",
    "    include_heads=[0, 1, 2, 3, 4, 5],\n",
    "    display_mode=\"light\",\n",
    "    html_action=\"return\",\n",
    ")\n",
    "\n",
    "with open(\"model_view.html\", \"w\") as file:\n",
    "    file.write(html_model_view.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head View\n",
    "html_head_view = head_view(\n",
    "    truncated_attention_matrix,\n",
    "    truncated_tokens,\n",
    "    # include_layers=[5],\n",
    "    html_action=\"return\",\n",
    ")\n",
    "\n",
    "with open(\"head_view.html\", \"w\") as file:\n",
    "    file.write(html_head_view.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron View\n",
    "model_type = \"bert\"\n",
    "\n",
    "show(model, model_type, tokenizer, display_mode=\"dark\", layer=5, head=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ee069c3f083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize REG token -> Tricky?\n",
    "#   DONE Why the row vs column attention differs? -> What the matrix actually represents\n",
    "# Include one example patient and visualize the attention matrix -> Include the exact concept token\n",
    "# Some sort of markers to separate visits and special tokens\n",
    "# Libraries used for attention visualization -> Amrit suggestion\n",
    "# Visualize the gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
