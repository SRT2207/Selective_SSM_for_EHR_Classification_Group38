{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File: XGBoost.ipynb\n",
    "Code to train and evaluate an XGBoost model on MIMIC-IV FHIR dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def ProjectObjectives():\n",
    "    \"\"\"\n",
    "    __Objectives__\n",
    "    0. Import data and separate unique visit tokens\n",
    "    1. Reduce the number of features (manual selection, hierarchy aggregation)\n",
    "    2. Create frequency features from event tokens\n",
    "    3. Include num_visits, youngest and oldest age, and maybe time\n",
    "    4. Use label column to create the prediction objective\n",
    "    5. Train XGBoost model and evaluate on test dataset\n",
    "    >>> All objectives successful\n",
    "    \"\"\"\n",
    "    return ProjectObjectives.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies and define useful constants\n",
    "import os\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import (\n",
    "    csr_matrix,\n",
    "    hstack,\n",
    "    load_npz,\n",
    "    save_npz,\n",
    "    vstack,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "%matplotlib inline\n",
    "\n",
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "TASK = \"multi_v2\"\n",
    "NUM_PATIENTS = \"all\"\n",
    "TASK_DURATION = \"readmission_1month\"\n",
    "DATA_ROOT = f\"{ROOT}/odyssey/data/bigbird_data\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/patient_sequences/patient_sequences_2048_{TASK}.parquet\"\n",
    "ID_PATH = f\"{DATA_ROOT}/patient_id_dict/dataset_2048_{TASK}.pkl\"\n",
    "FREQ_MATRIX_TRAIN = f\"{DATA_ROOT}/patient_freq_matrix/{TASK}/patient_freq_matrix_finetune_{TASK_DURATION}.npz\"\n",
    "FREQ_MATRIX_TEST = f\"{DATA_ROOT}/patient_freq_matrix/{TASK}/patient_freq_matrix_test_{TASK_DURATION}.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_parquet(DATA_PATH)\n",
    "# dataset.rename(columns={\"label_length_of_stay_1week\": \"label\"}, inplace=True)\n",
    "# dataset[\"label\"] = dataset[\"label\"].astype(int)\n",
    "# dataset.rename(columns={f'all_conditions': 'label'}, inplace=True)\n",
    "dataset.rename(columns={f\"label_{TASK_DURATION}\": \"label\"}, inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to truncate event tokens based on cutoff\n",
    "def truncate_tokens(row):\n",
    "    return row[\"event_tokens_2048\"][: row[f\"cutoff_{TASK_DURATION}\"]]\n",
    "\n",
    "\n",
    "dataset = dataset[dataset[f\"cutoff_{TASK_DURATION}\"] != -1]\n",
    "dataset[\"event_tokens_2048\"] = dataset.apply(truncate_tokens, axis=1)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = pickle.load(open(ID_PATH, \"rb\"))\n",
    "train_data = dataset.loc[\n",
    "    dataset[\"patient_id\"].isin(patient_ids[\"finetune\"][\"few_shot\"][NUM_PATIENTS])\n",
    "]\n",
    "test_data = dataset.loc[dataset[\"patient_id\"].isin(patient_ids[\"test\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique set of all possible tokens, including special tokens\n",
    "unique_event_tokens = set()\n",
    "\n",
    "for patient_event_tokens in tqdm(\n",
    "    dataset[\"event_tokens_2048\"].values,\n",
    "    desc=\"Loading Tokens\",\n",
    "    unit=\" Patients\",\n",
    "):\n",
    "    for event_token in patient_event_tokens:  # patient_event_tokens.split(\" \"):\n",
    "        unique_event_tokens.add(event_token)\n",
    "\n",
    "unique_event_tokens = list(unique_event_tokens)\n",
    "unique_event_tokens.sort(reverse=True)\n",
    "\n",
    "print(\n",
    "    f\"Complete list of unique event tokens\\nLength: {len(unique_event_tokens)}\\nHead: {unique_event_tokens[:30]}...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of tokens being used as features for the XGBoost\n",
    "# Note that feature 'id' will be dropped later on\n",
    "special_tokens = [\n",
    "    \"[CLS]\",\n",
    "    \"[PAD]\",\n",
    "    \"[REG]\",\n",
    "    \"[UNK]\",\n",
    "    \"[VS]\",\n",
    "    \"[VE]\",\n",
    "    \"[W_0]\",\n",
    "    \"[W_1]\",\n",
    "    \"[W_2]\",\n",
    "    \"[W_3]\",\n",
    "    *[f\"[M_{i}]\" for i in range(0, 13)],\n",
    "    \"[LT]\",\n",
    "]\n",
    "\n",
    "feature_event_tokens = [\"id\"] + [\n",
    "    token for token in unique_event_tokens if token not in special_tokens\n",
    "]\n",
    "\n",
    "print(len(feature_event_tokens), feature_event_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature frequency matrix for each feature token in the patient data\n",
    "# Since this will be saved to disk, you need only do it once\n",
    "\n",
    "\n",
    "def get_patient_frequency_matrix(\n",
    "    data: pd.DataFrame,\n",
    "    feature_event_tokens: List[str],\n",
    "    special_tokens: List[str],\n",
    "    output_path: str,\n",
    "    buffer_size: int = 50000,\n",
    ") -> None:\n",
    "    \"\"\"Calculate and save the patient frequency matrix.\"\"\"\n",
    "    patient_freq_matrix = None\n",
    "    matrix_buffer = []\n",
    "\n",
    "    for idx, patient in tqdm(\n",
    "        data.iterrows(),\n",
    "        desc=\"Loading Tokens\",\n",
    "        unit=\" Patients\",\n",
    "        total=len(data),\n",
    "    ):\n",
    "        patient_history = {token: 0 for token in feature_event_tokens}\n",
    "        patient_history[\"id\"] = idx\n",
    "\n",
    "        for event_token in patient[\"event_tokens_2048\"]:  # .split(\" \"):\n",
    "            if event_token not in special_tokens:\n",
    "                patient_history[event_token] += 1\n",
    "\n",
    "        matrix_buffer.append(list(patient_history.values()))\n",
    "\n",
    "        if len(matrix_buffer) >= buffer_size:\n",
    "            current_matrix = csr_matrix(\n",
    "                matrix_buffer,\n",
    "                shape=(len(matrix_buffer), len(feature_event_tokens)),\n",
    "            )\n",
    "\n",
    "            if patient_freq_matrix is None:\n",
    "                patient_freq_matrix = current_matrix\n",
    "            else:\n",
    "                patient_freq_matrix = vstack(\n",
    "                    [patient_freq_matrix, current_matrix],\n",
    "                    format=\"csr\",\n",
    "                )\n",
    "\n",
    "            matrix_buffer = []\n",
    "\n",
    "    if matrix_buffer:\n",
    "        current_matrix = csr_matrix(\n",
    "            matrix_buffer,\n",
    "            shape=(len(matrix_buffer), len(feature_event_tokens)),\n",
    "        )\n",
    "\n",
    "        if patient_freq_matrix is None:\n",
    "            patient_freq_matrix = current_matrix\n",
    "        else:\n",
    "            patient_freq_matrix = vstack(\n",
    "                [patient_freq_matrix, current_matrix],\n",
    "                format=\"csr\",\n",
    "            )\n",
    "\n",
    "    save_npz(output_path, patient_freq_matrix)\n",
    "    print(f\"Save & Done! Final Matrix Shape: {patient_freq_matrix.shape}\\n\")\n",
    "\n",
    "\n",
    "get_patient_frequency_matrix(\n",
    "    train_data, feature_event_tokens, special_tokens, output_path=FREQ_MATRIX_TRAIN\n",
    ")\n",
    "get_patient_frequency_matrix(\n",
    "    test_data, feature_event_tokens, special_tokens, output_path=FREQ_MATRIX_TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature frequency matrix from disk, generated by above code cell\n",
    "patient_freq_matrix = load_npz(FREQ_MATRIX_TRAIN)\n",
    "patient_freq_matrix_test = load_npz(FREQ_MATRIX_TEST)\n",
    "num_patients = patient_freq_matrix.shape[0] + patient_freq_matrix_test.shape[0]\n",
    "patient_freq_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_greater_than_zero(lst: List[int]) -> int:\n",
    "    \"\"\"Return the minimum positive number in the given list\"\"\"\n",
    "    positive_numbers = np.array(lst)[np.array(lst) > 0]\n",
    "\n",
    "    if len(positive_numbers) == 0:\n",
    "        return 0\n",
    "\n",
    "    min_positive = np.min(positive_numbers)\n",
    "    return min_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get extra features such as number of visits and age\n",
    "def add_age_to_freq_matrix(\n",
    "    data: pd.DataFrame,\n",
    "    patient_freq_matrix: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Add age feature to patient frequency matrix.\"\"\"\n",
    "    num_visits = data[\"num_visits\"].values\n",
    "    min_age = [\n",
    "        find_min_greater_than_zero(patient_age_tokens)\n",
    "        for patient_age_tokens in data[\"age_tokens_2048\"]\n",
    "    ]\n",
    "    max_age = [\n",
    "        np.max(patient_age_tokens) for patient_age_tokens in data[\"age_tokens_2048\"]\n",
    "    ]\n",
    "\n",
    "    # Add extra features to the frequency dataset\n",
    "    patient_freq_matrix = hstack(\n",
    "        [patient_freq_matrix, csr_matrix([num_visits, min_age, max_age]).T],\n",
    "        format=\"csr\",\n",
    "    )\n",
    "    return patient_freq_matrix[:, 1:]  # Drop id feature\n",
    "\n",
    "\n",
    "patient_freq_matrix = add_age_to_freq_matrix(train_data, patient_freq_matrix)\n",
    "patient_freq_matrix_test = add_age_to_freq_matrix(test_data, patient_freq_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intuition about the frequency of different features in the training dataset\n",
    "report_threshold = 1\n",
    "features_above_threshold = np.sum(\n",
    "    (patient_freq_matrix.getnnz(axis=0) >= report_threshold).astype(int),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"How many features have been reported for at least {report_threshold} patients?\\n\"\n",
    "    f\"{features_above_threshold} Features\",\n",
    ")\n",
    "\n",
    "# Plot the histogram of feature frequency\n",
    "# plt.hist(patient_freq_matrix.getnnz(axis=0), bins=range(num_patients+1), edgecolor='black')\n",
    "# plt.xlabel('Number of Nonzero Rows')\n",
    "# plt.ylabel('Number of Columns')\n",
    "# plt.title('Histogram of Nonzero Rows per Column')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick features to train the model on\n",
    "NUM_FEATURES = 20000\n",
    "features_sorted_by_freq = np.argsort(-patient_freq_matrix.getnnz(axis=0))\n",
    "selected_features = features_sorted_by_freq[: NUM_FEATURES + 1]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom labels, here death in 12 M, not needed if dataset already has labels\n",
    "# data[\"label\"] = (\n",
    "#     (data[\"death_after_end\"] >= 0) & (data[\"death_after_end\"] <= 31)\n",
    "# ).astype(int)\n",
    "#\n",
    "# print(f\"Total positive labels: {sum(data['label'])} out of {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model training\n",
    "X = vstack([patient_freq_matrix, patient_freq_matrix_test])\n",
    "Y = hstack(\n",
    "    [np.array(train_data[\"label\"].tolist()), np.array(test_data[\"label\"].tolist())]\n",
    ")\n",
    "\n",
    "# Optional, Scale features. Didn't improve performance\n",
    "# scaler = MaxAbsScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# Optional, Split data into train and test sets if dataset is not already split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, Y, test_size=0.2, stratify=Y, random_state=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test based on original dataset definitions\n",
    "X_train = patient_freq_matrix\n",
    "X_test = patient_freq_matrix_test\n",
    "\n",
    "y_train = np.array(train_data[\"label\"].tolist())\n",
    "y_test = np.array(test_data[\"label\"].tolist())\n",
    "\n",
    "patient_indices_train = y_train != -1\n",
    "patient_indices_test = y_test != -1\n",
    "\n",
    "X_train = X_train[patient_indices_train]\n",
    "X_test = X_test[patient_indices_test]\n",
    "y_train = y_train[patient_indices_train]\n",
    "y_test = y_test[patient_indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"unweighted\": [], \"weighted1\": [], \"weighted2\": [], \"weighted3\": []}\n",
    "weights = {\n",
    "    \"unweighted\": np.ones(shape=20),\n",
    "    \"weighted1\": np.array(\n",
    "        [\n",
    "            5.69327441,\n",
    "            7.42882028,\n",
    "            10.13231198,\n",
    "            11.49395911,\n",
    "            16.40469558,\n",
    "            11.0384684,\n",
    "            15.53901244,\n",
    "            10.03937008,\n",
    "            12.32068141,\n",
    "            12.45844666,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "            50.0,\n",
    "        ],\n",
    "    ).tolist(),\n",
    "    \"weighted2\": np.array(\n",
    "        [\n",
    "            1.0,\n",
    "            1.30484142,\n",
    "            1.77969851,\n",
    "            2.01886617,\n",
    "            2.88141663,\n",
    "            1.93886112,\n",
    "            2.72936299,\n",
    "            1.76337365,\n",
    "            2.16407651,\n",
    "            2.1882744,\n",
    "            339.421875,\n",
    "            60.34166667,\n",
    "            164.56818182,\n",
    "            156.28057554,\n",
    "            30.50983146,\n",
    "            145.79194631,\n",
    "            66.84,\n",
    "            88.66530612,\n",
    "            35.55319149,\n",
    "            35.32195122,\n",
    "        ],\n",
    "    ).tolist(),\n",
    "    \"weighted3\": np.array(\n",
    "        [\n",
    "            1.0,\n",
    "            1.14229656,\n",
    "            1.33405341,\n",
    "            1.4208681,\n",
    "            1.6974736,\n",
    "            1.39242993,\n",
    "            1.65207838,\n",
    "            1.3279208,\n",
    "            1.47108005,\n",
    "            1.47928172,\n",
    "            18.42340563,\n",
    "            7.76798987,\n",
    "            12.82841307,\n",
    "            12.50122296,\n",
    "            5.52357054,\n",
    "            12.07443358,\n",
    "            8.17557337,\n",
    "            9.41622568,\n",
    "            5.9626497,\n",
    "            5.94322734,\n",
    "        ],\n",
    "    ).tolist(),\n",
    "}\n",
    "\n",
    "# Train a separate model for each label\n",
    "for weighting_scheme in models:\n",
    "    for i in range(y_train.shape[1]):\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            random_state=23,\n",
    "            scale_pos_weight=weights[weighting_scheme][i],\n",
    "        )\n",
    "        model.fit(X_train, y_train[:, i])\n",
    "        models[weighting_scheme].append(model)\n",
    "        print(f\"Model saved for label: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, X):\n",
    "    \"\"\"\n",
    "    Generate both class predictions and probabilities.\n",
    "    \"\"\"\n",
    "    pred = model.predict(X)\n",
    "    prob = model.predict_proba(X)[:, 1]\n",
    "    return pred, prob\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Calculate and return performance metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"AUROC\": roc_auc_score(y_true, y_prob),\n",
    "        \"Average Precision Score\": average_precision_score(y_true, y_prob),\n",
    "    }\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    metrics[\"AUC-PR\"] = auc(recall, precision)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, label_prefix):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for a dataset.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=f\"{label_prefix} AUROC={roc_auc_score(y_true, y_prob):.3f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "def assess_model_performance(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    X_all,\n",
    "    Y_all,\n",
    "    plot=False,\n",
    "    print_metrics=False,\n",
    "    return_test_metrics=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Assess model performance across training, testing, and all data.\n",
    "    \"\"\"\n",
    "    # Generate Predictions\n",
    "    y_train_pred, y_train_prob = generate_predictions(model, X_train)\n",
    "    y_test_pred, y_test_prob = generate_predictions(model, X_test)\n",
    "    all_data_pred, all_data_prob = generate_predictions(model, X_all)\n",
    "\n",
    "    # Calculate Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_prob)\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_prob)\n",
    "    all_data_metrics = calculate_metrics(Y_all, all_data_pred, all_data_prob)\n",
    "\n",
    "    # Print Metrics\n",
    "    if print_metrics:\n",
    "        for metric_name in train_metrics:\n",
    "            print(\n",
    "                f\"{metric_name}\\nTrain: {train_metrics[metric_name]:.5f}  |  \"\n",
    "                f\"Test: {test_metrics[metric_name]:.5f}  |  \"\n",
    "                f\"All Data: {all_data_metrics[metric_name]:.5f}\\n\",\n",
    "            )\n",
    "\n",
    "    # Plot ROC Curve if requested\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plot_roc_curve(y_train, y_train_prob, \"Train\")\n",
    "        plot_roc_curve(y_test, y_test_prob, \"Test\")\n",
    "        # plot_roc_curve(Y_all, all_data_prob, \"All Data\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if return_test_metrics:\n",
    "        return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:, 15].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "\n",
    "for weighting_scheme in models:\n",
    "    print(f\"\\n{weighting_scheme}\")\n",
    "    print(\n",
    "        assess_model_performance(\n",
    "            models[weighting_scheme][i],\n",
    "            X_train,\n",
    "            y_train[:, i],\n",
    "            X_test,\n",
    "            y_test[:, i],\n",
    "            X,\n",
    "            Y.toarray()[:, i],\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to scale positive weights\n",
    "total_negative = len(y_train) - sum(y_train)\n",
    "total_positive = sum(y_train)\n",
    "\n",
    "scale_pos_weight = total_negative / (\n",
    "    total_positive // 1\n",
    ")  # 1.5 for two_weeks and 4 for one_month | use sqrt if extremely imbalanced\n",
    "\n",
    "# Single XGBoost Classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=23,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ASSESS MODEL PERFORMANCE ###\n",
    "\n",
    "# Predict labels for train, test, and all data\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_train_prob = xgb_model.predict_proba(X_train)[:, 1]\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "y_test_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Balanced Accuracy\n",
    "y_train_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
    "y_test_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# F1 Score\n",
    "y_train_f1 = f1_score(y_train, y_train_pred)\n",
    "y_test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Precision\n",
    "y_train_precision = precision_score(y_train, y_train_pred)\n",
    "y_test_precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "# Recall\n",
    "y_train_recall = recall_score(y_train, y_train_pred)\n",
    "y_test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "# AUROC\n",
    "y_train_auroc = roc_auc_score(y_train, y_train_prob)\n",
    "y_test_auroc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# AUC-PR (Area Under the Precision-Recall Curve)\n",
    "y_train_p, y_train_r, _ = precision_recall_curve(y_train, y_train_prob)\n",
    "y_test_p, y_test_r, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "\n",
    "y_train_auc_pr = auc(y_train_r, y_train_p)\n",
    "y_test_auc_pr = auc(y_test_r, y_test_p)\n",
    "\n",
    "# Average Precision Score (APS)\n",
    "y_train_aps = average_precision_score(y_train, y_train_pred)\n",
    "y_test_aps = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "# Print Metrics\n",
    "print(\n",
    "    f\"Balanced Accuracy\\nTrain: {y_train_accuracy:.5f}  |  Test: {y_test_accuracy:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"F1 Score\\nTrain: {y_train_f1:.5f}  |  Test: {y_test_f1:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Precision\\nTrain: {y_train_precision:.5f}  |  Test: {y_test_precision:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Recall\\nTrain: {y_train_recall:.5f}  |  Test: {y_test_recall:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"AUROC\\nTrain: {y_train_auroc:.5f}  |  Test: {y_test_auroc:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"AUC-PR\\nTrain: {y_train_auc_pr:.5f}  |  Test: {y_test_auc_pr:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision Score\\nTrain: {y_train_aps:.5f}  |  Test: {y_test_aps:.5f}\\n\",\n",
    ")\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_prob)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "\n",
    "# Plot Information\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train AUROC={y_train_auroc:.3f}\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test AUROC={y_test_auroc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL, one needs to match feature ids to actual FHIR features before interpretation\n",
    "# Assess which features are the most important\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "# Create a list of tuples (feature, importance) and sort it by importance in descending order\n",
    "sorted_importances = sorted(\n",
    "    zip(selected_features, feature_importances),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "# Display the top 10 most important features\n",
    "top_features = sorted_importances[:10]\n",
    "for feature, importance in top_features:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current predictions, labels, and probabilities to disk\n",
    "np.save(f\"{DATA_ROOT}/xgboost_y_test_pred_{TASK}.npy\", y_test_pred)\n",
    "np.save(f\"{DATA_ROOT}/xgboost_y_test_pred_{TASK}_labels.npy\", y_test)\n",
    "np.save(\n",
    "    f\"{DATA_ROOT}/xgboost_y_test_pred_{TASK}_prob.npy\",\n",
    "    xgb_model.predict_proba(X_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRIPT FOR K-FOLD VALIDATION ###\n",
    "N_FOLDS = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=23)\n",
    "\n",
    "# Initialize lists to store performance metrics for each fold\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "aurocs = []\n",
    "auc_prs = []\n",
    "average_precision_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in tqdm(\n",
    "    stratified_kfold.split(X, Y),\n",
    "    desc=f\"{N_FOLDS}-Fold Validation\",\n",
    "    unit=\" Model(s)\",\n",
    "):\n",
    "    # Get the relevant train and test data\n",
    "    X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "    y_train_fold, y_test_fold = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Create a new XGBoost model for each fold\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        random_state=23,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "    )\n",
    "\n",
    "    # Train the model on the training fold\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Predict on the test fold\n",
    "    y_pred_fold = xgb_model.predict(X_test_fold)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy_fold = balanced_accuracy_score(y_test_fold, y_pred_fold)\n",
    "    f1_fold = f1_score(y_test_fold, y_pred_fold)\n",
    "    precision_fold = precision_score(y_test_fold, y_pred_fold)\n",
    "    recall_fold = recall_score(y_test_fold, y_pred_fold)\n",
    "    auroc_fold = roc_auc_score(y_test_fold, y_pred_fold)\n",
    "    p_fold, r_fold, _ = precision_recall_curve(y_test_fold, y_pred_fold)\n",
    "    auc_pr_fold = auc(r_fold, p_fold)\n",
    "    average_precision_score_fold = average_precision_score(y_test_fold, y_pred_fold)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy_fold)\n",
    "    f1_scores.append(f1_fold)\n",
    "    precisions.append(precision_fold)\n",
    "    recalls.append(recall_fold)\n",
    "    aurocs.append(auroc_fold)\n",
    "    auc_prs.append(auc_pr_fold)\n",
    "    average_precision_scores.append(average_precision_score_fold)\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(f\"Average Balanced Accuracy: {sum(accuracy_scores) / N_FOLDS:.5f}\")\n",
    "print(f\"Average F1 Score: {sum(f1_scores) / N_FOLDS:.5f}\")\n",
    "print(f\"Average Precision: {sum(precisions) / N_FOLDS:.5f}\")\n",
    "print(f\"Average Recall: {sum(recalls) / N_FOLDS:.5f}\")\n",
    "print(f\"Average AUROC: {sum(aurocs) / N_FOLDS:.5f}\")\n",
    "print(f\"Average AUC-PR: {sum(auc_prs) / N_FOLDS:.5f}\")\n",
    "print(f\"Average Precision Score: {sum(average_precision_scores) / N_FOLDS:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate labels for the dataset if necessary\n",
    "# labes_for_given_token = []; given_token = '8135'; de = []\n",
    "#\n",
    "# for idx, patient in tqdm(dataset.iterrows(), desc=\"Loading Tokens\", unit=\" Patients\"):\n",
    "#     if given_token in list(patient['event_tokens_2048'].split(' ')):\n",
    "#         # print(patient); break\n",
    "#         de.append(patient['label'])\n",
    "#\n",
    "# print(f\"Total Occurrences: {len(labes_for_given_token)}, Positive Labels: {sum(labes_for_given_token)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
